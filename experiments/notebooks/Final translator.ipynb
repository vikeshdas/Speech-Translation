{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5767f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 17:56:06.022477: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-01 17:56:06.022508: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18913ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4262fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4e00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35be589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    #it return object->tokenizer and tokenize form of data(sentesces)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d579dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    print(\"type is:\",type(x))\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2329bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y, en_max_len=None, fr_max_len=None):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x, en_max_len)\n",
    "    preprocess_y = pad(preprocess_y, fr_max_len)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac8a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247eb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shpae=dimention of english data \n",
    "    \n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,optimizer=Adam(learning_rate),metrics=['accuracy'])\n",
    "    \n",
    "    #compile\n",
    "    model.fit(preproc_english_sentences_train, preproc_french_sentences_train,batch_size=1024, epochs=1, validation_split=0.2)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518295eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test accurecy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e1322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model,preproc_english_sentences_test,preproc_french_sentences_test):\n",
    "    \n",
    "    result = model.evaluate(preproc_english_sentences_test,preproc_french_sentences_test, batch_size=1024)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d884b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,path):\n",
    "    print(\"inside save_model\")\n",
    "    model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c8b9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model=tf.keras.models.load_model(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38953c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b39d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    \n",
    "    index_to_words = {}\n",
    "    for word, _id in tokenizer.word_index.items():\n",
    "        index_to_words[_id] = word\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    \n",
    "    res=\"\" \n",
    "    for prediction in np.argmax(logits,1):\n",
    "        if prediction!=0:\n",
    "            res=res+\" \"+index_to_words[prediction]\n",
    "        \n",
    "    return res;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "575bee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_predictions(model,sentence,french_tokenizer,english_tokenizer,preproc_english_sentences):\n",
    "    \n",
    "    sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=preproc_english_sentences.shape[-1], padding='post')\n",
    "    sentence=model.predict(sentence, len(sentence))\n",
    "    #reshaping because after prediction size is (1,21,345)\n",
    "    sentence = sentence.reshape(21,345)\n",
    "    res=logits_to_text(sentence,french_tokenizer)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43e68740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3615b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "english_sentences = '/home/jai/Documents/projects/translator/data/small_vocab_en'\n",
    "french_sentences = '/home/jai/Documents/projects/translator/data/small_vocab_fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7f950c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type is: <class 'list'>\n",
      "type is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "english_sentences =load_data(english_sentences)\n",
    "french_sentences =load_data(french_sentences)\n",
    "    \n",
    "#english_tokenizer stores key and value where key is a number and value is unique word in englsih data set\n",
    "#same for french_tokenizer\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =preprocess(english_sentences, french_sentences)\n",
    "\n",
    "preproc_english_sentences_train, preproc_english_sentences_test, preproc_french_sentences_train, preproc_french_sentences_test = train_test_split(preproc_english_sentences, preproc_french_sentences, test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6870b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for jai, vik in english_tokenizer.word_index.items():\n",
    "#     print(jai, vik)\n",
    "#len(english_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7fafebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 17:15:09.352344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-01 17:15:09.352369: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-01 17:15:09.352385: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vikesh-hp-pc): /proc/driver/nvidia/version does not exist\n",
      "2022-05-01 17:15:09.352706: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-01 17:15:09.810184: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 77s 1s/step - loss: 2.9086 - accuracy: 0.4408 - val_loss: 1.9547 - val_accuracy: 0.5096\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 128)           25600     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               198144    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 21, 256)           296448    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 21, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 345)           176985    \n",
      "=================================================================\n",
      "Total params: 828,761\n",
      "Trainable params: 828,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "#preproc_english_sentences_train.shape->dimention of preproc_english_sentences_train\n",
    "#preproc_french_sentences_train.shape[1]->number of collumn in preproc_french_sentences_train.\n",
    "#len(english_tokenizer.word_index)->number of unique words in english dataset\n",
    "\n",
    "model=train_model(preproc_english_sentences_train.shape,preproc_french_sentences_train.shape[1],len(english_tokenizer.word_index)+1,len(french_tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78c3da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 9s 204ms/step - loss: 1.9537 - accuracy: 0.5095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9536659717559814, 0.509480357170105]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test accurecy\n",
    "test_accuracy(model,preproc_english_sentences_test,preproc_french_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a93b66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "sentence = 'he saw a old yellow truck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14c95396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' il les les les et'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions(model,sentence,french_tokenizer,english_tokenizer,preproc_english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367254e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
